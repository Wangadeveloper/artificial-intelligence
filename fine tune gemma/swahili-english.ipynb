{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9906717,"sourceType":"datasetVersion","datasetId":6086466},{"sourceId":9973122,"sourceType":"datasetVersion","datasetId":6135957},{"sourceId":85986,"sourceType":"modelInstanceVersion","modelInstanceId":72246,"modelId":78150}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q -U keras-nlp datasets\n!pip install -q -U keras\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n\nimport keras_nlp\nimport keras\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Set the backbend before importing Keras\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n# Avoid memory fragmentation on JAX backend.\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run at half precision.\n#keras.config.set_floatx(\"bfloat16\")\n\n# Training Configurations\ntoken_limit = 256\nnum_data_limit = 100\nlora_name = \"swahili\"\nlora_rank = 4\nlr_value = 1e-4\ntrain_epoch = 10\nmodel_id = \"gemma2_instruct_2b_en\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)\ngemma_lm.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(gemma_lm.generate(\"translate: mama amezaa mtoto, to english?\",max_length=256))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ndf=pd.read_csv('/kaggle/input/swahili-dataset-for-gemma/final1_swahili_english.csv')\n\ndf.columns=['swahili','english']\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport json\n\n# Load the CSV file\ncsv_file_path = '/kaggle/input/swahili-dataset-for-gemma/final1_swahili_english.csv'\ndata = pd.read_csv(csv_file_path)\n\n# Convert DataFrame to JSON\njson_data = data.to_dict(orient='records')\n\n# Save JSON data to file\njson_file_path = 'swahili_english_translations.json'\nwith open(json_file_path, 'w', encoding='utf-8') as json_file:\n    json.dump(json_data, json_file, ensure_ascii=False, indent=4)\n\nprint(f\"CSV file has been converted to JSON and saved as {json_file_path}.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the JSON file\ndataset = load_dataset(\"json\", data_files=\"swahili_english_translations.json\")\n\n# Preview the dataset\nprint(dataset['train'][1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LoRA_rank = 2 # you can modify this \n# Enable LoRA for the model and set the LoRA rank to 2,4,...\ngemma_lm.backbone.enable_lora(rank=LoRA_rank)\ngemma_lm.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Limit the input sequence length to 256 (to control memory usage).\ngemma_lm.preprocessor.sequence_length = 256\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.01,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = keras_nlp.models.GemmaTokenizer.from_preset(model_id)\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"json\", data_files=\"swahili_english_translations.json\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply the tokenization function\n\ndata = dataset.with_format(\n    \"np\", columns=[\"kiswahili\", \"english\"], output_all_columns=False\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = []\nfor x in data['train']:  # Iterating over the 'train' split\n    item = f\"<start_of_turn>user\\n{x['kiswahili']}<end_of_turn>\\n<start_of_turn>model\\n{x['english']}<end_of_turn>\"\n    length = len(tokenizer(item))\n    # skip data if the token length is longer than our limit\n    if length < token_limit:\n        train.append(item)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_train=train[0:3000]\nvallid=train[3001:]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(new_train))\nprint(new_train[0])\nprint(len(vallid))\n#print(vallid[2001])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = gemma_lm.fit(new_train, epochs=train_epoch, batch_size=1)\n\nimport matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(gemma_lm.generate(\"translate: mama amezaa mtoto, to english?\",max_length=256))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def text_gen(prompt):\n    input = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n    output = gemma_lm.generate(input, max_length=token_limit)\n    print(\"\\nGemma output:\")\n    print(output)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\n# Save the model to a file\nwith open('model.pkl', 'wb') as file:\n    pickle.dump(gemma_lm, file)\n\nprint(\"Model saved as model.pkl\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}