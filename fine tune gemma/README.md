the notebook here preparation involved the following steps in its preparation:

Dataset Creation/Curation: where i crafted and  curated the dataset used for fine-tuning. processed data can be publically obtained from kaggle https://www.kaggle.com/datasets/wangapa106g/swahili-english-data, preprocessing steps, and also taking into consideration issues related to data quality and cultural sensitivity.
Fine-tuning Gemma:the notebook on kaggle have detailed explanation of fine-tuning approach, including hyperparameter choices, training procedures, and any techniques used to enhance performance (e.g., few-shot prompting, retrieval-augmented generation).
Inference and Evaluation: Demosntration on how to run inference with on the fine-tuned model and evaluation on its performance is also captured.
