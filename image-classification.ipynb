{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "315796ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T07:56:41.324657Z",
     "iopub.status.busy": "2025-04-03T07:56:41.324288Z",
     "iopub.status.idle": "2025-04-03T07:56:54.482065Z",
     "shell.execute_reply": "2025-04-03T07:56:54.481008Z"
    },
    "papermill": {
     "duration": 13.164622,
     "end_time": "2025-04-03T07:56:54.483941",
     "exception": false,
     "start_time": "2025-04-03T07:56:41.319319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ac2e66f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T07:56:54.492806Z",
     "iopub.status.busy": "2025-04-03T07:56:54.492286Z",
     "iopub.status.idle": "2025-04-03T07:56:54.557295Z",
     "shell.execute_reply": "2025-04-03T07:56:54.556105Z"
    },
    "papermill": {
     "duration": 0.07141,
     "end_time": "2025-04-03T07:56:54.559165",
     "exception": false,
     "start_time": "2025-04-03T07:56:54.487755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 1️⃣ Load Metadata\n",
    "# ==============================\n",
    "train_labels = pd.read_csv(\"/kaggle/input/image-matching-challenge-2025/train_labels.csv\")\n",
    "\n",
    "def parse_vector(vec_str):\n",
    "    return list(map(float, vec_str.split(\";\")))\n",
    "\n",
    "train_labels[\"rotation_matrix\"] = train_labels[\"rotation_matrix\"].apply(parse_vector)\n",
    "train_labels[\"translation_vector\"] = train_labels[\"translation_vector\"].apply(parse_vector)\n",
    "\n",
    "grouped_scenes = train_labels.groupby([\"dataset\", \"scene\"])[\"image\"].apply(list).to_dict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36e528a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T07:56:54.567163Z",
     "iopub.status.busy": "2025-04-03T07:56:54.566786Z",
     "iopub.status.idle": "2025-04-03T07:56:54.571383Z",
     "shell.execute_reply": "2025-04-03T07:56:54.570259Z"
    },
    "papermill": {
     "duration": 0.010585,
     "end_time": "2025-04-03T07:56:54.573188",
     "exception": false,
     "start_time": "2025-04-03T07:56:54.562603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 2️⃣ Load Pretrained EfficientNet\n",
    "# ==============================\n",
    "def load_model():\n",
    "    \"\"\"Loads the pretrained EfficientNet model for embedding extraction.\"\"\"\n",
    "    model = models.efficientnet_b0(pretrained=True)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32cc533a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T07:56:54.581290Z",
     "iopub.status.busy": "2025-04-03T07:56:54.580915Z",
     "iopub.status.idle": "2025-04-03T07:56:54.586906Z",
     "shell.execute_reply": "2025-04-03T07:56:54.585682Z"
    },
    "papermill": {
     "duration": 0.012053,
     "end_time": "2025-04-03T07:56:54.588675",
     "exception": false,
     "start_time": "2025-04-03T07:56:54.576622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 3️⃣ Image Preprocessing\n",
    "# ==============================\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Loads and preprocesses an image.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b168490",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T07:56:54.596698Z",
     "iopub.status.busy": "2025-04-03T07:56:54.596348Z",
     "iopub.status.idle": "2025-04-03T07:56:54.601351Z",
     "shell.execute_reply": "2025-04-03T07:56:54.600259Z"
    },
    "papermill": {
     "duration": 0.010914,
     "end_time": "2025-04-03T07:56:54.603077",
     "exception": false,
     "start_time": "2025-04-03T07:56:54.592163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 4️⃣ EfficientNet Embedding Extraction\n",
    "# ==============================\n",
    "def get_image_embedding(image_path, model):\n",
    "    \"\"\"Extracts an image embedding using EfficientNet.\"\"\"\n",
    "    image_tensor = preprocess_image(image_path)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = model.features(image_tensor)  # Extract features\n",
    "        embedding = torch.flatten(features)  # Flatten feature map\n",
    "    \n",
    "    return embedding.numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a50fb0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T07:56:54.611014Z",
     "iopub.status.busy": "2025-04-03T07:56:54.610684Z",
     "iopub.status.idle": "2025-04-03T07:56:54.615245Z",
     "shell.execute_reply": "2025-04-03T07:56:54.614320Z"
    },
    "papermill": {
     "duration": 0.010405,
     "end_time": "2025-04-03T07:56:54.616866",
     "exception": false,
     "start_time": "2025-04-03T07:56:54.606461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 5️⃣ Compute Embeddings for a Scene\n",
    "# ==============================\n",
    "def compute_scene_embeddings(image_paths, model):\n",
    "    \"\"\"Computes embeddings for all images in a scene.\"\"\"\n",
    "    embeddings = {}\n",
    "    for img in image_paths:\n",
    "        embeddings[img] = get_image_embedding(img, model)\n",
    "    \n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ddb0999",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T07:56:54.624893Z",
     "iopub.status.busy": "2025-04-03T07:56:54.624553Z",
     "iopub.status.idle": "2025-04-03T07:56:54.630603Z",
     "shell.execute_reply": "2025-04-03T07:56:54.629596Z"
    },
    "papermill": {
     "duration": 0.011961,
     "end_time": "2025-04-03T07:56:54.632371",
     "exception": false,
     "start_time": "2025-04-03T07:56:54.620410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 6️⃣ Clustering with DBSCAN (Using Embeddings)\n",
    "# ==============================\n",
    "def cluster_images(embeddings, eps=0.5, min_samples=3):\n",
    "    \"\"\"Clusters images into scenes using DBSCAN on normalized embeddings.\"\"\"\n",
    "    \n",
    "    image_paths = list(embeddings.keys())\n",
    "    embedding_matrix = np.array([embeddings[img] for img in image_paths])\n",
    "\n",
    "    # Normalize embeddings to use cosine similarity properly\n",
    "    normalized_embeddings = normalize(embedding_matrix, axis=1)\n",
    "\n",
    "    # Apply DBSCAN with cosine similarity metric\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric=\"cosine\")\n",
    "    labels = dbscan.fit_predict(normalized_embeddings)\n",
    "\n",
    "    # Group images by scene clusters\n",
    "    scene_groups = {}\n",
    "    outliers = []\n",
    "\n",
    "    for img, label in zip(image_paths, labels):\n",
    "        if label == -1:\n",
    "            outliers.append(img)\n",
    "        else:\n",
    "            scene_groups.setdefault(label, []).append(img)\n",
    "\n",
    "    return scene_groups, outliers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cee7cd5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T07:56:54.640501Z",
     "iopub.status.busy": "2025-04-03T07:56:54.640110Z",
     "iopub.status.idle": "2025-04-03T07:56:54.645841Z",
     "shell.execute_reply": "2025-04-03T07:56:54.644782Z"
    },
    "papermill": {
     "duration": 0.011891,
     "end_time": "2025-04-03T07:56:54.647729",
     "exception": false,
     "start_time": "2025-04-03T07:56:54.635838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 7️⃣ Outlier Filtering via Cosine Similarity\n",
    "# ==============================\n",
    "def filter_outliers(embeddings, outlier_threshold=0.5):\n",
    "    \"\"\"Filters out images with low cosine similarity to the mean scene embedding.\"\"\"\n",
    "    \n",
    "    image_paths = list(embeddings.keys())\n",
    "    embedding_matrix = np.array([embeddings[img] for img in image_paths])\n",
    "\n",
    "    # Compute the mean embedding for the scene\n",
    "    mean_embedding = np.mean(embedding_matrix, axis=0)\n",
    "    \n",
    "    filtered_outliers = []\n",
    "    for img, embedding in embeddings.items():\n",
    "        sim_score = 1 - cosine(embedding, mean_embedding)  # Cosine similarity (higher is better)\n",
    "        if sim_score < outlier_threshold:\n",
    "            filtered_outliers.append(img)\n",
    "    \n",
    "    return filtered_outliers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed7339ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T07:56:54.655938Z",
     "iopub.status.busy": "2025-04-03T07:56:54.655564Z",
     "iopub.status.idle": "2025-04-03T07:56:54.660832Z",
     "shell.execute_reply": "2025-04-03T07:56:54.659753Z"
    },
    "papermill": {
     "duration": 0.011534,
     "end_time": "2025-04-03T07:56:54.662728",
     "exception": false,
     "start_time": "2025-04-03T07:56:54.651194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 8️⃣ Load Images from Dataset\n",
    "# ==============================\n",
    "def load_images_from_scene(dataset, scene):\n",
    "    \"\"\"Loads images from a given dataset and scene.\"\"\"\n",
    "    base_path = f\"/kaggle/input/image-matching-challenge-2025/train/{dataset}/\"\n",
    "    images = grouped_scenes.get((dataset, scene), [])\n",
    "    return [os.path.join(base_path, img) for img in images if os.path.exists(os.path.join(base_path, img))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35cc65db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T07:56:54.671512Z",
     "iopub.status.busy": "2025-04-03T07:56:54.671146Z",
     "iopub.status.idle": "2025-04-03T07:56:54.678631Z",
     "shell.execute_reply": "2025-04-03T07:56:54.677499Z"
    },
    "papermill": {
     "duration": 0.014064,
     "end_time": "2025-04-03T07:56:54.680489",
     "exception": false,
     "start_time": "2025-04-03T07:56:54.666425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 9️⃣ Generate Submission File\n",
    "# ==============================\n",
    "def save_submission_file(results, output_path=\"submission.csv\"):\n",
    "    \"\"\"Saves results to a CSV submission file.\"\"\"\n",
    "    submission_rows = []\n",
    "\n",
    "    for dataset, clusters in results.items():\n",
    "        for cluster_id, images in clusters.items():\n",
    "            for img in images:\n",
    "                # Handle outliers first\n",
    "                if cluster_id == \"outliers\":\n",
    "                    rotation = \"nan;nan;nan;nan;nan;nan;nan;nan;nan\"\n",
    "                    translation = \"nan;nan;nan\"\n",
    "                else:\n",
    "                    # Check if the image exists in train_labels\n",
    "                    image_data = train_labels[train_labels[\"image\"] == img]\n",
    "                    if image_data.empty:\n",
    "                        # If the image is not found, assign nan to pose data\n",
    "                        rotation = \"nan;nan;nan;nan;nan;nan;nan;nan;nan\"\n",
    "                        translation = \"nan;nan;nan\"\n",
    "                    else:\n",
    "                        # Get rotation and translation for the image\n",
    "                        rotation = \";\".join(map(str, image_data[\"rotation_matrix\"].values[0]))\n",
    "                        translation = \";\".join(map(str, image_data[\"translation_vector\"].values[0]))\n",
    "\n",
    "                # Append the row for the current image\n",
    "                submission_rows.append([dataset, f\"cluster{cluster_id}\", img, rotation, translation])\n",
    "\n",
    "    # Create DataFrame from the submission rows and save to CSV\n",
    "    submission_df = pd.DataFrame(submission_rows, columns=[\"dataset\", \"scene\", \"image\", \"rotation_matrix\", \"translation_vector\"])\n",
    "    submission_df.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Submission saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b6eb465",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T07:56:54.688751Z",
     "iopub.status.busy": "2025-04-03T07:56:54.688399Z",
     "iopub.status.idle": "2025-04-03T08:02:06.742692Z",
     "shell.execute_reply": "2025-04-03T08:02:06.741622Z"
    },
    "papermill": {
     "duration": 312.060346,
     "end_time": "2025-04-03T08:02:06.744425",
     "exception": false,
     "start_time": "2025-04-03T07:56:54.684079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
      "100%|██████████| 20.5M/20.5M [00:00<00:00, 158MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Dataset: ETs, Scene: ET\n",
      "\n",
      "Processing Dataset: ETs, Scene: another_ET\n",
      "\n",
      "Processing Dataset: ETs, Scene: outliers\n",
      "\n",
      "Processing Dataset: amy_gardens, Scene: peach\n",
      "\n",
      "Processing Dataset: fbk_vineyard, Scene: vineyard_split_1\n",
      "\n",
      "Processing Dataset: fbk_vineyard, Scene: vineyard_split_2\n",
      "\n",
      "Processing Dataset: fbk_vineyard, Scene: vineyard_split_3\n",
      "\n",
      "Processing Dataset: imc2023_haiper, Scene: bike\n",
      "\n",
      "Processing Dataset: imc2023_haiper, Scene: chairs\n",
      "\n",
      "Processing Dataset: imc2023_haiper, Scene: fountain\n",
      "\n",
      "Processing Dataset: imc2023_heritage, Scene: cyprus\n",
      "\n",
      "Processing Dataset: imc2023_heritage, Scene: dioscuri\n",
      "\n",
      "Processing Dataset: imc2023_heritage, Scene: outliers\n",
      "\n",
      "Processing Dataset: imc2023_heritage, Scene: wall\n",
      "\n",
      "Processing Dataset: imc2023_theather_imc2024_church, Scene: church\n",
      "\n",
      "Processing Dataset: imc2023_theather_imc2024_church, Scene: kyiv-puppet-theater\n",
      "\n",
      "Processing Dataset: imc2024_dioscuri_baalshamin, Scene: baalshamin\n",
      "\n",
      "Processing Dataset: imc2024_dioscuri_baalshamin, Scene: dioscuri\n",
      "\n",
      "Processing Dataset: imc2024_dioscuri_baalshamin, Scene: outliers\n",
      "\n",
      "Processing Dataset: imc2024_lizard_pond, Scene: lizard\n",
      "\n",
      "Processing Dataset: imc2024_lizard_pond, Scene: outliers\n",
      "\n",
      "Processing Dataset: imc2024_lizard_pond, Scene: pond\n",
      "\n",
      "Processing Dataset: pt_brandenburg_british_buckingham, Scene: brandenburg_gate\n",
      "\n",
      "Processing Dataset: pt_brandenburg_british_buckingham, Scene: british_museum\n",
      "\n",
      "Processing Dataset: pt_brandenburg_british_buckingham, Scene: buckingham_palace\n",
      "\n",
      "Processing Dataset: pt_piazzasanmarco_grandplace, Scene: grand_place_brussels\n",
      "\n",
      "Processing Dataset: pt_piazzasanmarco_grandplace, Scene: piazza_san_marco\n",
      "\n",
      "Processing Dataset: pt_sacrecoeur_trevi_tajmahal, Scene: sacre_coeur\n",
      "\n",
      "Processing Dataset: pt_sacrecoeur_trevi_tajmahal, Scene: taj_mahal\n",
      "\n",
      "Processing Dataset: pt_sacrecoeur_trevi_tajmahal, Scene: trevi_fountain\n",
      "\n",
      "Processing Dataset: pt_stpeters_stpauls, Scene: st_pauls_cathedral\n",
      "\n",
      "Processing Dataset: pt_stpeters_stpauls, Scene: st_peters_square\n",
      "\n",
      "Processing Dataset: stairs, Scene: stairs_split_1\n",
      "\n",
      "Processing Dataset: stairs, Scene: stairs_split_2\n",
      "✅ Submission saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 🔟 Run the Pipeline\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    model = load_model()\n",
    "    results = {}\n",
    "\n",
    "    for (dataset, scene), image_list in grouped_scenes.items():\n",
    "        print(f\"\\nProcessing Dataset: {dataset}, Scene: {scene}\")\n",
    "        \n",
    "        image_paths = load_images_from_scene(dataset, scene)\n",
    "        \n",
    "        if not image_paths:\n",
    "            print(\"No valid images found.\")\n",
    "            continue\n",
    "\n",
    "        # Compute embeddings\n",
    "        embeddings = compute_scene_embeddings(image_paths, model)\n",
    "\n",
    "        # Cluster images into scenes\n",
    "        scene_clusters, outliers = cluster_images(embeddings)\n",
    "\n",
    "        # Further refine outliers\n",
    "        refined_outliers = filter_outliers(embeddings)\n",
    "\n",
    "        # Store results\n",
    "        results[dataset] = {k: v for k, v in scene_clusters.items()}\n",
    "        results[dataset][\"outliers\"] = refined_outliers\n",
    "\n",
    "    # Save submission file\n",
    "    save_submission_file(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd36e7c",
   "metadata": {
    "papermill": {
     "duration": 0.005256,
     "end_time": "2025-04-03T08:02:06.755520",
     "exception": false,
     "start_time": "2025-04-03T08:02:06.750264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11655853,
     "sourceId": 91498,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 330.748861,
   "end_time": "2025-04-03T08:02:09.275247",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-03T07:56:38.526386",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
